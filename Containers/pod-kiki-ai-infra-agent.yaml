apiVersion: v1
kind: Pod
metadata:
  name: kiki-ai-infra-agent
spec:
  hostNetwork: true
  restartPolicy: Always
  network:
    - name: podman
  containers:
    - name: llama-server
      image: ghcr.io/ggerganov/llama.cpp:server
      args: ["--model","/models/data.gguf",
             "--host","0.0.0.0","--port","8080",
             "--ctx-size","4096","--n-gpu-layers","0",
              # ▼ CPU 성능 관련 옵션
             "--threads","8",        # 물리코어(or 논리코어) 수에 맞게 조정
             "--batch-size","512",    # 한 번에 처리할 토큰 배치 크기
             "--ubatch-size","32",    # 내부 마이크로 배치, 너무 크면 레이턴시 증가
             "--numa","distribute"    # 멀티 소켓이면 NUMA 분산
           ]
      volumeMounts:
        - name: models
          mountPath: /models/data.gguf
          readOnly: true
    - name: ansible-agent
      image: localhost/kiki-ai-infra-agent:latest
      env:
        - name: MODEL_URL
          value: http://127.0.0.1:8080/v1
        - name: API_KEY
          value: sk-noauth
        - name: WORK_DIR
          value: /work
      volumeMounts:
        - name: sshkeys
          mountPath: /home/agent/.ssh
          readOnly: true
        - name: work
          mountPath: /work
      ports:
        - containerPort: 8082
          hostPort: 8082
  volumes:
    - name: models
      hostPath:
        path: /root/models/data.gguf  ## model 파일이 있는 위치(호스트 컴퓨터)
        type: File
    - name: sshkeys
      hostPath:
        path: /root/.ssh
        type: Directory
    - name: work
      hostPath:
        path: /data/agent-work
        type: DirectoryOrCreate
